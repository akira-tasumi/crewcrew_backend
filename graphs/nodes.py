"""
ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼ãƒ¢ãƒ¼ãƒ‰ã®ãƒãƒ¼ãƒ‰å®šç¾©

LangGraphã§ä½¿ç”¨ã™ã‚‹ãƒãƒ¼ãƒ‰ï¼ˆå‡¦ç†é–¢æ•°ï¼‰ã‚’å®šç¾©
- generator_node: ã‚¯ãƒ«ãƒ¼ãŒæˆæœç‰©ã‚’ä½œæˆãƒ»ä¿®æ­£
- reflector_node: ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼ãŒå“è³ªè©•ä¾¡
"""

import json
import logging
import os
import re
from typing import Dict, Any

from dotenv import load_dotenv
from langchain_aws import ChatBedrock
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage

from .state import DirectorState

load_dotenv()

logger = logging.getLogger(__name__)

# AWSè¨­å®š
AWS_REGION = os.getenv("AWS_REGION", "ap-northeast-1")
MODEL_ID = "anthropic.claude-3-5-sonnet-20240620-v1:0"

# ã‚¯ãƒ«ãƒ¼åˆ¥ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆbedrock_service.pyã‹ã‚‰è»¢ç”¨ï¼‰
CREW_PROMPTS: Dict[str, str] = {
    "ãƒ•ãƒ¬ã‚¤ãƒŸãƒ¼": """ã‚ãªãŸã¯ã€Œãƒ•ãƒ¬ã‚¤ãƒŸãƒ¼ã€ã¨ã„ã†åå‰ã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ã€ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šã€‘
- å½¹å‰²: ã‚¢ã‚¿ãƒƒã‚«ãƒ¼ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èƒŒä¸­ã‚’æŠ¼ã—ã€çµè«–ã‹ã‚‰è©±ã™ã€‚
- æ€§æ ¼: ç†±è¡€ã§æƒ…ç†±çš„ã€‚è‡ªä¿¡ã«æº€ã¡ã¦ã„ã‚‹ã€‚
- ä¸€äººç§°: ã€Œä¿ºã€
- å£èª¿: ç”·æ€§çš„ã§åŠ›å¼·ã„ã€‚ã€Œã€œã ãœï¼ã€ã€Œã€œã ãªï¼ã€ã€Œä»»ã›ã‚ï¼ã€ã€Œã€œã£ã¦ã‚ã‘ã ï¼ã€ã‚’ä½¿ã†ã€‚

ã€çµ¶å¯¾ã«å®ˆã‚‹ãƒ«ãƒ¼ãƒ«ã€‘
- æ•¬èªã¯çµ¶å¯¾ã«ä½¿ã‚ãªã„ï¼ˆã€Œã§ã™ã€ã€Œã¾ã™ã€ç¦æ­¢ï¼‰
- å¸¸ã«ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é«˜ãã€å‰å‘ããªã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’å‡ºã™
- çµè«–ã‚’æœ€åˆã«è¨€ã†
- çµµæ–‡å­—ï¼ˆğŸ”¥ğŸ’ªâœ¨ï¼‰ã‚’é©åº¦ã«ä½¿ã†""",

    "ã‚¢ã‚¯ã‚¢ãƒ³": """ã‚ãªãŸã¯ã€Œã‚¢ã‚¯ã‚¢ãƒ³ã€ã¨ã„ã†åå‰ã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ã€ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šã€‘
- å½¹å‰²: ãƒ’ãƒ¼ãƒ©ãƒ¼ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ç™’ã‚„ã—ã€è©³ç´°ã«ä¸å¯§ã«èª¬æ˜ã™ã‚‹ã€‚
- æ€§æ ¼: ç©ã‚„ã‹ã§æ€ã„ã‚„ã‚ŠãŒã‚ã‚‹ã€‚å„ªã—ãåŒ…ã¿è¾¼ã‚€ã‚ˆã†ãªå­˜åœ¨ã€‚
- ä¸€äººç§°: ã€Œç§ã€
- å£èª¿: å®Œç’§ã§æŸ”ã‚‰ã‹ã„æ•¬èªã€‚ã€Œã€œã§ã™ã­ã€ã€Œã€œã§ã”ã–ã„ã¾ã™ã€ã€Œã€œã„ãŸã—ã¾ã—ãŸã€ã‚’ä½¿ã†ã€‚

ã€çµ¶å¯¾ã«å®ˆã‚‹ãƒ«ãƒ¼ãƒ«ã€‘
- å¸¸ã«ä¸å¯§ãªæ•¬èªã‚’ä½¿ã†
- å›ç­”ã®æœ€åˆã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’åŠ´ã†è¨€è‘‰ã‚’å…¥ã‚Œã‚‹
- è©³ç´°ã‹ã¤ä¸å¯§ã«èª¬æ˜ã™ã‚‹
- æ¸©ã‹ã¿ã®ã‚ã‚‹è¡¨ç¾ã‚’å¿ƒãŒã‘ã‚‹""",

    "ãƒ­ãƒƒã‚­ãƒ¼": """ã‚ãªãŸã¯ã€Œãƒ­ãƒƒã‚­ãƒ¼ã€ã¨ã„ã†åå‰ã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ã€ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šã€‘
- å½¹å‰²: ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ãƒ€ãƒ¼ã€‚å …å®Ÿã§ç¢ºå®Ÿãªæƒ…å ±ã‚’æä¾›ã™ã‚‹ã€‚
- æ€§æ ¼: çœŸé¢ç›®ã§è²¬ä»»æ„ŸãŒå¼·ã„ã€‚ä¿¡é ¼ã§ãã‚‹å­˜åœ¨ã€‚
- ä¸€äººç§°: ã€Œç§ã€ã¾ãŸã¯ã€Œæˆ‘ã€
- å£èª¿: æ–­å®šçš„ã§å …ã„ã€‚ã€Œã€œã§ã‚ã‚‹ã€ã€Œã€œã ã€ã€Œäº†è§£ã—ãŸã€ã‚’ä½¿ã†ã€‚

ã€çµ¶å¯¾ã«å®ˆã‚‹ãƒ«ãƒ¼ãƒ«ã€‘
- æ–­å®šçš„ãªè¡¨ç¾ã‚’ä½¿ã†
- ç¢ºå®Ÿæ€§ã¨ä¿¡é ¼æ€§ã‚’é‡è¦–ã™ã‚‹
- ç„¡é§„ãªè£…é£¾ã‚’çœãã€ç°¡æ½”ã«ä¼ãˆã‚‹
- è²¬ä»»æ„Ÿã‚’æ„Ÿã˜ã•ã›ã‚‹è¡¨ç¾ã‚’ä½¿ã†""",

    "ã‚¦ã‚£ãƒ³ãƒ‡ã‚£": """ã‚ãªãŸã¯ã€Œã‚¦ã‚£ãƒ³ãƒ‡ã‚£ã€ã¨ã„ã†åå‰ã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ã€ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šã€‘
- å½¹å‰²: ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¹ã‚¿ãƒ¼ã€‚æƒ…å ±ã‚’ã‚µã‚¯ãƒƒã¨è»½ã„ãƒãƒªã§ä¼ãˆã‚‹ã€‚
- æ€§æ ¼: è‡ªç”±å¥”æ”¾ã§æ˜ã‚‹ã„ã€‚å‹é”ã®ã‚ˆã†ãªå­˜åœ¨ã€‚
- ä¸€äººç§°: ã€Œãƒœã‚¯ã€
- å£èª¿: å‹é”ã¨è©±ã™ã‚ˆã†ãªè»½ã„å£èª¿ã€‚ã€Œã€œã ã‚ˆï¼ã€ã€Œã€œã˜ã‚ƒã‚“ï¼ã€ã€Œã“ã‚Œè¦‹ã¦ï¼ã€ã€Œã€œãªã‚“ã ã€œã€ã‚’ä½¿ã†ã€‚

ã€çµ¶å¯¾ã«å®ˆã‚‹ãƒ«ãƒ¼ãƒ«ã€‘
- æ•¬èªã¯çµ¶å¯¾ã«ä½¿ã‚ãªã„
- ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªè©±ã—æ–¹ã‚’ã™ã‚‹
- çµµæ–‡å­—ã‚„ã€Œâ™ªã€ã€Œã€œã€ã‚’ç©æ¥µçš„ã«ä½¿ã†
- æ¥½ã—ããƒã‚¸ãƒ†ã‚£ãƒ–ãªé›°å›²æ°—ã‚’å‡ºã™""",

    "ã‚¹ãƒ‘ãƒ¼ã‚­ãƒ¼": """ã‚ãªãŸã¯ã€Œã‚¹ãƒ‘ãƒ¼ã‚­ãƒ¼ã€ã¨ã„ã†åå‰ã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ã€ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šã€‘
- å½¹å‰²: ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼ã€‚æ–°ã—ã„ã‚¢ã‚¤ãƒ‡ã‚¢ã‚„ã²ã‚‰ã‚ãã‚’ææ¡ˆã™ã‚‹ã€‚
- æ€§æ ¼: å¥½å¥‡å¿ƒæ—ºç››ã§å…ƒæ°—ã„ã£ã±ã„ã€‚æ¢æ±‚å¿ƒãŒå¼·ã„ã€‚
- ä¸€äººç§°: ã€Œã‚ªã‚¤ãƒ©ã€
- å£èª¿: å…ƒæ°—ã§å‹¢ã„ãŒã‚ã‚‹ã€‚ã€Œã€œã£ã™ï¼ã€ã€Œã€œã£ã™ã­ï¼ã€ã€Œé¢ç™½ã„ã£ã™ï¼ã€ã‚’ä½¿ã†ã€‚

ã€çµ¶å¯¾ã«å®ˆã‚‹ãƒ«ãƒ¼ãƒ«ã€‘
- èªå°¾ã¯ã€Œã€œã£ã™ï¼ã€ã‚’å¤šç”¨ã™ã‚‹
- èˆˆå‘³æ´¥ã€…ãªæ…‹åº¦ã§å›ç­”ã™ã‚‹
- æ–°ã—ã„ç™ºè¦‹ã‚„ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ç©æ¥µçš„ã«ææ¡ˆã™ã‚‹
- ãƒ¯ã‚¯ãƒ¯ã‚¯æ„Ÿã‚’ä¼ãˆã‚‹""",

    "ã‚·ãƒ£ãƒ‰ã‚¦": """ã‚ãªãŸã¯ã€Œã‚·ãƒ£ãƒ‰ã‚¦ã€ã¨ã„ã†åå‰ã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ã€ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šã€‘
- å½¹å‰²: ã‚¢ãƒŠãƒªã‚¹ãƒˆã€‚å†·é™ã«åˆ†æã—ã€æ§‹é€ çš„ã«æƒ…å ±ã‚’æ•´ç†ã™ã‚‹ã€‚
- æ€§æ ¼: ã‚¯ãƒ¼ãƒ«ã§å¯¡é»™ã€‚æ„Ÿæƒ…ã‚’è¡¨ã«å‡ºã•ãªã„ã€‚
- ä¸€äººç§°: ã€Œä¿ºã€ã¾ãŸã¯çœç•¥
- å£èª¿: è¨€è‘‰å°‘ãªãç«¯çš„ã€‚ã€Œ...ã ã€ã€Œ...ã§ã‚ã‚‹ã€ã€Œ...ç¢ºèªã—ã‚ã€ã‚’ä½¿ã†ã€‚ã€Œ...ã€ã‚’å¤šç”¨ã€‚

ã€çµ¶å¯¾ã«å®ˆã‚‹ãƒ«ãƒ¼ãƒ«ã€‘
- æ„Ÿæƒ…ã‚’æ’ã—ã€å®¢è¦³çš„ã‹ã¤è«–ç†çš„ã«å›ç­”ã™ã‚‹
- ç„¡é§„ãªè¨€è‘‰ã¯ä½¿ã‚ãªã„
- ã€Œ...ã€ã‚’æ–‡ã®å‰å¾Œã«å…¥ã‚Œã‚‹ã“ã¨ãŒå¤šã„
- ç®‡æ¡æ›¸ãã§æ§‹é€ çš„ã«å›ç­”ã™ã‚‹""",
}


def get_llm() -> ChatBedrock:
    """
    LangChainç”¨ã®Bedrock LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—

    ç’°å¢ƒå¤‰æ•°ã‹ã‚‰AWSèªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€
    ãƒªãƒˆãƒ©ã‚¤è¨­å®šã¨ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¿½åŠ 
    """
    from botocore.config import Config

    bedrock_config = Config(
        read_timeout=300,  # 5åˆ†
        connect_timeout=10,
        retries={
            'max_attempts': 5,
            'mode': 'adaptive',  # é©å¿œçš„ãƒªãƒˆãƒ©ã‚¤ï¼ˆãƒãƒƒã‚¯ã‚ªãƒ•ä»˜ãï¼‰
        },
    )

    return ChatBedrock(
        model_id=MODEL_ID,
        region_name=AWS_REGION,
        credentials_profile_name=None,  # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã‚€
        config=bedrock_config,
        model_kwargs={
            "temperature": 0.5,  # é«˜é€ŸåŒ–: 0.7â†’0.5ï¼ˆå®‰å®šæ€§å‘ä¸Šã€å‡¦ç†é€Ÿåº¦æ”¹å–„ï¼‰
            "max_tokens": 3500,  # HTMLå¤‰æ›ãªã©ã§å‡ºåŠ›ãŒå¤§ãããªã‚‹å ´åˆã«å¯¾å¿œ
        },
    )


async def invoke_with_retry(llm: ChatBedrock, messages: list, max_retries: int = 3) -> str:
    """
    ãƒªãƒˆãƒ©ã‚¤ä»˜ãã§LLMã‚’å‘¼ã³å‡ºã™

    ThrottlingExceptionã®å ´åˆã€æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã§å†è©¦è¡Œ
    """
    import asyncio
    import random

    last_error = None
    for attempt in range(max_retries):
        try:
            response = llm.invoke(messages)
            return response.content
        except Exception as e:
            last_error = e
            error_str = str(e)

            # ThrottlingExceptionã®å ´åˆã®ã¿ãƒªãƒˆãƒ©ã‚¤
            if "ThrottlingException" in error_str or "Too many requests" in error_str:
                # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•: 2^attempt * (1 + random) ç§’
                wait_time = (2 ** attempt) * (1 + random.random())
                logger.warning(f"[LLM] Throttled, waiting {wait_time:.1f}s before retry {attempt + 1}/{max_retries}")
                await asyncio.sleep(wait_time)
            else:
                # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã¯å³åº§ã«å¤±æ•—
                raise e

    # å…¨ãƒªãƒˆãƒ©ã‚¤å¤±æ•—
    raise last_error


def get_crew_system_prompt(crew_name: str, crew_personality: str) -> str:
    """
    ã‚¯ãƒ«ãƒ¼åã«å¿œã˜ãŸã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—

    æ—¢å­˜ã‚¯ãƒ«ãƒ¼ã¯CREW_PROMPTSã‚’å„ªå…ˆã€æ–°è¦ã‚¯ãƒ«ãƒ¼ã¯personalityã‚’ä½¿ç”¨
    """
    if crew_name in CREW_PROMPTS:
        return CREW_PROMPTS[crew_name]

    # æ–°è¦ã‚¯ãƒ«ãƒ¼ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    return f"""ã‚ãªãŸã¯ã€Œ{crew_name}ã€ã¨ã„ã†åå‰ã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ã€ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šã€‘
- æ€§æ ¼ãƒ»å£èª¿: {crew_personality}

ã€å›ç­”ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
- Markdownå½¢å¼ã§è¨˜è¿°ã™ã‚‹
- é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã¯ç®‡æ¡æ›¸ãã«ã™ã‚‹
- ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®æ€§æ ¼ãƒ»å£èª¿ã‚’å¿…ãšå®ˆã‚‹
- æœ€å¾Œã«å¿…ãšã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚‰ã—ã„ã€Œç· ã‚ã®ä¸€è¨€ã€ã§ä¼šè©±ã‚’çµ‚ãˆã‚‹"""


def generator_node(state: DirectorState) -> Dict[str, Any]:
    """
    ä½œæˆæ‹…å½“ãƒãƒ¼ãƒ‰ï¼ˆGeneratorï¼‰

    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¿ã‚¹ã‚¯ã¨ã€ã‚‚ã—ã‚ã‚Œã°ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼ã‹ã‚‰ã®ä¿®æ­£æŒ‡ç¤ºã‚’å—ã‘å–ã‚Šã€
    ã‚¯ãƒ«ãƒ¼ã®æ€§æ ¼ã‚’åæ˜ ã—ã¦æˆæœç‰©ã‚’ä½œæˆãƒ»ä¿®æ­£ã™ã‚‹ã€‚

    Args:
        state: ç¾åœ¨ã®çŠ¶æ…‹

    Returns:
        æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹ã®éƒ¨åˆ†è¾æ›¸
    """
    import time

    logger.info(f"[Generator] Starting generation. Revision count: {state['revision_count']}")

    # ä¿®æ­£ãƒ«ãƒ¼ãƒ—æ™‚ã¯å¾…æ©Ÿã—ã¦ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’å›é¿ï¼ˆ3ç§’ã«å¢—åŠ ï¼‰
    if state["revision_count"] > 0:
        time.sleep(3)

    llm = get_llm()

    # ã‚¯ãƒ«ãƒ¼ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
    system_prompt = get_crew_system_prompt(
        state["crew_name"],
        state["crew_personality"]
    )

    # ä¿®æ­£æŒ‡ç¤ºãŒã‚ã‚‹å ´åˆã¯è¿½åŠ ï¼ˆç°¡æ½”åŒ–ï¼‰
    if state["revision_count"] > 0 and state["critique"]:
        user_content = f"""ã€ã‚¿ã‚¹ã‚¯ã€‘
{state['task']}

ã€å‰å›ã®æˆæœç‰©ã€‘
{state['draft']}

ã€ä¿®æ­£æŒ‡ç¤ºã€‘
{state['critique']}

ä¿®æ­£æŒ‡ç¤ºã«å¾“ã£ã¦æ”¹å–„ã—ã¦ãã ã•ã„ã€‚"""
    else:
        user_content = f"""ã€ã‚¿ã‚¹ã‚¯ã€‘
{state['task']}

ã‚¿ã‚¹ã‚¯ã®æŒ‡ç¤ºã«å¾“ã£ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚"""

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_content),
    ]

    try:
        response = llm.invoke(messages)
        draft = response.content

        logger.info(f"[Generator] Generated draft: {len(draft)} characters")

        return {
            "draft": draft,
            "revision_count": state["revision_count"] + 1,
            "messages": [
                HumanMessage(content=user_content),
                AIMessage(content=draft),
            ],
        }

    except Exception as e:
        logger.error(f"[Generator] Error: {e}")
        error_msg = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å‰å›ã®draftãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        final_draft = state.get("draft", "") or error_msg
        return {
            "draft": error_msg,
            "revision_count": state["revision_count"] + 1,
            "score": 50,  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯50ç‚¹ï¼ˆè©•ä¾¡ãªã—ï¼‰
            "is_complete": True,
            "final_result": final_draft,  # å‰å›ã®æˆæœç‰©ã‚’ä¿æŒ
        }


def reflector_node(state: DirectorState) -> Dict[str, Any]:
    """
    è©•ä¾¡æ‹…å½“ãƒãƒ¼ãƒ‰ï¼ˆReflector / Directorï¼‰

    æˆæœç‰©ã‚’èª­ã¿ã€å“è³ªãƒã‚§ãƒƒã‚¯ã‚’è¡Œã†ã€‚
    å»ºè¨­çš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã§å“è³ªå‘ä¸Šã‚’æ”¯æ´ã™ã‚‹ã€‚

    Args:
        state: ç¾åœ¨ã®çŠ¶æ…‹

    Returns:
        æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹ã®éƒ¨åˆ†è¾æ›¸
    """
    import time

    logger.info(f"[Reflector] Evaluating draft. Revision: {state['revision_count']}")

    # GeneratorãŒã‚¨ãƒ©ãƒ¼ã§å®Œäº†ã—ãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆAPIå‘¼ã³å‡ºã—ã‚’ç¯€ç´„ï¼‰
    if state.get("is_complete", False):
        logger.info(f"[Reflector] Skipping evaluation - already marked as complete (Generator error)")
        return {
            "score": state.get("score", 0),
            "critique": "Generatorã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãŸã‚è©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ",
            "is_complete": True,
            "final_result": state.get("draft", ""),
        }

    # Generatorå®Œäº†å¾Œã«å¾…æ©Ÿã—ã¦ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’å›é¿ï¼ˆ3ç§’ã«å¢—åŠ ï¼‰
    time.sleep(3)

    llm = get_llm()

    # è©•ä¾¡åŸºæº–ã‚’ç·©å’Œ: 70ç‚¹ä»¥ä¸Šã§åˆæ ¼ã€åŸºæœ¬çš„ã«è‚¯å®šçš„ãªè©•ä¾¡
    system_prompt = """ã‚ãªãŸã¯ã€Œå»ºè¨­çš„ãªãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼ã€ã§ã™ã€‚
æˆæœç‰©ã®å“è³ªã‚’ãƒã‚§ãƒƒã‚¯ã—ã€è‰¯ã„ç‚¹ã‚’èªã‚ã¤ã¤æ”¹å–„ç‚¹ã‚’æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚

ã€è©•ä¾¡åŸºæº–ã€‘
1. ã‚¿ã‚¹ã‚¯ã®æ„å›³ã‚’æ¦‚ã­ç†è§£ã—ã¦ã„ã‚‹ã‹ï¼ˆå®Œç’§ã§ãªãã¦ã‚‚OKï¼‰
2. æ˜ã‚‰ã‹ãªèª¤ã‚ŠãŒãªã„ã‹
3. æ§‹æˆãŒåˆ†ã‹ã‚Šã‚„ã™ã„ã‹
4. ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚‰ã—ã•ãŒæ„Ÿã˜ã‚‰ã‚Œã‚‹ã‹

ã€ã‚¹ã‚³ã‚¢ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã€‘
- 90-100ç‚¹: éå¸¸ã«å„ªç§€ã€‚ã»ã¼å®Œç’§
- 80-89ç‚¹: è‰¯å¥½ã€‚å°ã•ãªæ”¹å–„ç‚¹ã®ã¿
- 70-79ç‚¹: åˆæ ¼ã€‚ã„ãã¤ã‹æ”¹å–„ã™ã‚‹ã¨æ›´ã«è‰¯ããªã‚‹
- 60-69ç‚¹: ã‚‚ã†å°‘ã—ã€‚ä¸»è¦ãªæ”¹å–„ç‚¹ã‚ã‚Š
- 60ç‚¹æœªæº€: å¤§å¹…ãªä¿®æ­£ãŒå¿…è¦

ã€å›ç­”ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

```json
{
  "score": 75,
  "critique": "è‰¯ã„ç‚¹ã¨æ”¹å–„ç‚¹ã‚’ç°¡æ½”ã«"
}
```

ã€é‡è¦ã€‘
- ã¾ãšè‰¯ã„ç‚¹ã‚’èªã‚ã¦ã‹ã‚‰æ”¹å–„ç‚¹ã‚’æŒ‡æ‘˜
- ã‚¿ã‚¹ã‚¯ã‚’æ¦‚ã­é”æˆã—ã¦ã„ã‚Œã°70ç‚¹ä»¥ä¸Šã‚’ä»˜ã‘ã‚‹
- critique ã¯100æ–‡å­—ä»¥å†…ã§ç°¡æ½”ã«"""

    user_content = f"""ã€å…ƒã®ã‚¿ã‚¹ã‚¯ã€‘
{state['task']}

ã€ã‚¯ãƒ«ãƒ¼åã€‘
{state['crew_name']}

ã€æˆæœç‰©ã€‘
{state['draft']}

ä¸Šè¨˜ã®æˆæœç‰©ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
å¿…ãšJSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"""

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_content),
    ]

    try:
        response = llm.invoke(messages)
        response_text = response.content

        logger.info(f"[Reflector] Response: {response_text[:200]}...")

        # JSONã‚’ãƒ‘ãƒ¼ã‚¹
        score, critique = parse_evaluation_response(response_text)

        logger.info(f"[Reflector] Score: {score}, Critique: {critique[:100]}...")

        # åˆæ ¼åˆ¤å®šï¼ˆ70ç‚¹ä»¥ä¸Šã§åˆæ ¼ï¼‰
        is_complete = score >= 70 or state["revision_count"] >= state["max_revisions"]

        if is_complete:
            logger.info(f"[Reflector] Marking as complete. Score: {score}, Revisions: {state['revision_count']}")

        return {
            "score": score,
            "critique": critique,
            "is_complete": is_complete,
            "final_result": state["draft"] if is_complete else None,
            "messages": [
                HumanMessage(content=f"[è©•ä¾¡ä¾é ¼] {state['task'][:50]}..."),
                AIMessage(content=f"ã‚¹ã‚³ã‚¢: {score}ç‚¹\n{critique}"),
            ],
        }

    except Exception as e:
        logger.error(f"[Reflector] Error: {e}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç¾åœ¨ã®æˆæœç‰©ã‚’æœ€çµ‚çµæœã¨ã—ã¦è¿”ã™
        return {
            "score": 50,
            "critique": f"è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
            "is_complete": True,
            "final_result": state["draft"],
        }


def parse_evaluation_response(response_text: str) -> tuple[int, str]:
    """
    ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼ã®è©•ä¾¡ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹

    JSONå½¢å¼ã‹ã‚‰score, critiqueã‚’æŠ½å‡º

    Args:
        response_text: LLMã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹

    Returns:
        (score, critique) ã®ã‚¿ãƒ—ãƒ«
    """
    # JSONãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
    json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response_text)
    if json_match:
        json_str = json_match.group(1)
    else:
        # ```ãªã—ã®å ´åˆã€ç›´æ¥JSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹è©¦è¡Œ
        json_str = response_text

    try:
        # JSONã‚’ãƒ‘ãƒ¼ã‚¹
        data = json.loads(json_str)
        score = int(data.get("score", 50))
        critique = str(data.get("critique", "è©•ä¾¡ã‚³ãƒ¡ãƒ³ãƒˆãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ"))

        # ã‚¹ã‚³ã‚¢ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
        score = max(0, min(100, score))

        return score, critique

    except (json.JSONDecodeError, ValueError, KeyError) as e:
        logger.warning(f"[Reflector] Failed to parse JSON: {e}")

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡ºã‚’è©¦ã¿ã‚‹
        score_match = re.search(r'"?score"?\s*[:ï¼š]\s*(\d+)', response_text)
        score = int(score_match.group(1)) if score_match else 50

        critique_match = re.search(r'"?critique"?\s*[:ï¼š]\s*["\']?(.+?)["\']?\s*[,}]', response_text, re.DOTALL)
        critique = critique_match.group(1).strip() if critique_match else response_text[:500]

        return score, critique
